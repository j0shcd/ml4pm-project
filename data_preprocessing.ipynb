{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# hv.renderer('bokeh').theme = 'dark_minimal'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_root = Path(r\"C:\\Users\\Turquin\\Documents\\MLFPMA - Machine Learning for Predictive Maintenance Application\\Project\\Dataset\") # Raw string works without escaping \\\n",
    "dataset_root = Path(\"./Dataset\")\n",
    "\n",
    "@dataclass\n",
    "class Case():\n",
    "    info: pd.DataFrame\n",
    "    measurements: pd.DataFrame\n",
    "\n",
    "\n",
    "class RawDataset():\n",
    "    def __init__(self, root, unit = \"VG4\", load_training=False, load_synthetic=False, load_anomalies=False) -> None:\n",
    "        read_pq_file = lambda f: pq.read_table(root / f).to_pandas()\n",
    "        \n",
    "        cases = {\n",
    "            \"test\": [f\"{unit}_generator_data_testing_real_measurements.parquet\", root / f\"{unit}_generator_data_testing_real_info.csv\" ], \n",
    "        }\n",
    "\n",
    "        if load_training:\n",
    "            cases = {\n",
    "                **cases,\n",
    "                \"train\": [f\"{unit}_generator_data_training_measurements.parquet\", root / f\"{unit}_generator_data_training_info.csv\" ], \n",
    "            }\n",
    "\n",
    "        if load_synthetic:\n",
    "            cases = {\n",
    "                **cases,\n",
    "                \"test_s01\": [f\"{unit}_generator_data_testing_synthetic_01_measurements.parquet\", root / f\"{unit}_generator_data_testing_synthetic_01_info.csv\"], \n",
    "                \"test_s02\": [f\"{unit}_generator_data_testing_synthetic_02_measurements.parquet\", root / f\"{unit}_generator_data_testing_synthetic_02_info.csv\"]\n",
    "            }\n",
    "\n",
    "        if load_anomalies:\n",
    "            anomaly_folder = Path(\"synthetic_anomalies\")  # Relative path\n",
    "            subdataset = [\"01\", \"02\"]\n",
    "            anomaly_types = [\"a\", \"b\", \"c\"]\n",
    "            for anomaly in subdataset:\n",
    "                for subtype in anomaly_types:\n",
    "                    anomaly_key = f\"anomaly_{anomaly}_type_{subtype}\"\n",
    "                    anomaly_file = f\"{unit}_anomaly_{anomaly}_type_{subtype}.parquet\"\n",
    "                    full_anomaly_path = root / anomaly_folder / anomaly_file\n",
    "                    if full_anomaly_path.exists():\n",
    "                        cases[anomaly_key] = [anomaly_folder / anomaly_file, None]\n",
    "\n",
    "        \n",
    "        self.data_dict = dict()\n",
    "        \n",
    "        for id_c, c in cases.items():\n",
    "            # if you need to verify the parquet header:\n",
    "            # pq_rows = RawDataset.read_parquet_schema_df(root / c[0])\n",
    "            measurements = read_pq_file(c[0])\n",
    "            info = pd.read_csv(c[1]) if c[1] is not None else None\n",
    "            self.data_dict[id_c] = Case(info, measurements)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_parquet_schema_df(uri: str) -> pd.DataFrame:\n",
    "        \"\"\"Return a Pandas dataframe corresponding to the schema of a local URI of a parquet file.\n",
    "\n",
    "        The returned dataframe has the columns: column, pa_dtype\n",
    "        \"\"\"\n",
    "        # Ref: https://stackoverflow.com/a/64288036/\n",
    "        schema = pq.read_schema(uri, memory_map=True)\n",
    "        schema = pd.DataFrame(({\"column\": name, \"pa_dtype\": str(pa_dtype)} for name, pa_dtype in zip(schema.names, schema.types)))\n",
    "        schema = schema.reindex(columns=[\"column\", \"pa_dtype\"], fill_value=pd.NA)  # Ensures columns in case the parquet file has an empty dataframe.\n",
    "        return schema\n",
    "    \n",
    "\n",
    "rds_u4 = RawDataset(dataset_root, \"VG4\", load_synthetic=False, load_training=True)\n",
    "rds_u5 = RawDataset(dataset_root, \"VG5\", load_synthetic=True, load_training=True, load_anomalies=True)\n",
    "rds_u6 = RawDataset(dataset_root, \"VG6\", load_synthetic=True, load_training=True, load_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_anomaly_ground_truth(rds):\n",
    "    subdataset = [\"01\", \"02\"]\n",
    "    anomaly_types = [\"a\", \"b\", \"c\"]\n",
    "\n",
    "    results = []\n",
    "    for anomaly in subdataset:\n",
    "        test_s012 = rds.data_dict[f'test_s{anomaly}'].measurements\n",
    "\n",
    "        for subtype in anomaly_types:\n",
    "            anomaly_key = f\"anomaly_{anomaly}_type_{subtype}\"\n",
    "            labeled_df = rds.data_dict[anomaly_key].measurements\n",
    "            test_s012.loc[labeled_df['ground_truth'] == 1, anomaly_key] = 1\n",
    "        \n",
    "        test_s012['anomaly'] = (test_s012[[f'anomaly_{anomaly}_type_a',f'anomaly_{anomaly}_type_b',f'anomaly_{anomaly}_type_c']].max(axis=1) == 1).astype(int)\n",
    "        results.append(test_s012)\n",
    "\n",
    "    return results\n",
    "\n",
    "u5_s01, u5_s02 = add_anomaly_ground_truth(rds_u5)\n",
    "u6_s01, u6_s02 = add_anomaly_ground_truth(rds_u6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tot_activepower', 'charge', 'coupler_position',\n",
       "       'injector_01_opening', 'injector_02_opening',\n",
       "       'injector_03_opening', 'injector_04_opening',\n",
       "       'injector_05_opening', 'pump_calculated_flow',\n",
       "       'pump_pressure_diff', 'pump_rotspeed', 'turbine_pressure',\n",
       "       'turbine_rotspeed', 'water_primary_pump_01_opening',\n",
       "       'water_primary_pump_02_opening', 'timer_injector_opening',\n",
       "       'timer_turbine_on_off'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_control_vars(df):\n",
    "    return df[(df['control_signal'] == True) | (df['input_feature'] == True)].attribute_name.values\n",
    "\n",
    "get_control_vars(rds_u4.data_dict[\"train\"].info)\n",
    "get_control_vars(rds_u5.data_dict[\"train\"].info)\n",
    "get_control_vars(rds_u6.data_dict[\"train\"].info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operating_modes(df):\n",
    "    df_equilibrium_turbine_mode = df[df['equilibrium_turbine_mode'] == True]\n",
    "    df_equilibrium_pump_mode = df[df['equilibrium_pump_mode'] == True]\n",
    "    return df_equilibrium_turbine_mode, df_equilibrium_pump_mode\n",
    "\n",
    "# train sets\n",
    "u4_train_equil_turbine, u4_train_equil_pump = get_operating_modes(rds_u4.data_dict[\"train\"].measurements)\n",
    "u5_train_equil_turbine, u5_train_equil_pump = get_operating_modes(rds_u5.data_dict[\"train\"].measurements)\n",
    "u6_train_equil_turbine, u6_train_equil_pump = get_operating_modes(rds_u6.data_dict[\"train\"].measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synethetic test sets\n",
    "u5_s01_equil_turbine, u5_s01_equil_pump = get_operating_modes(u5_s01)\n",
    "u5_s02_equil_turbine, u5_s02_equil_pump = get_operating_modes(u5_s02)\n",
    "u6_s01_equil_turbine, u6_s01_equil_pump = get_operating_modes(u6_s01)\n",
    "u6_s02_equil_turbine, u6_s02_equil_pump = get_operating_modes(u6_s02)\n",
    "\n",
    "# real test sets\n",
    "u4_test_equil_turbine, u4_test_equil_pump = get_operating_modes(rds_u4.data_dict[\"test\"].measurements)\n",
    "u5_test_equil_turbine, u5_test_equil_pump = get_operating_modes(rds_u5.data_dict[\"test\"].measurements)\n",
    "u6_test_equil_turbine, u6_test_equil_pump = get_operating_modes(rds_u6.data_dict[\"test\"].measurements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples:\n",
    "\n",
    "For Unit 4:\n",
    "- train on u4_train_equil_pump, test on u4_test_equil_pump\n",
    "\n",
    "For Unit 5:\n",
    "- train on u5_train_equil_turbine, test on u5_s01_equil_turbine, u5_s02_equil_turbine, u5_test_equil_turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'u4_train_equil_turbine' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store u4_train_equil_turbine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "civil-426",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
