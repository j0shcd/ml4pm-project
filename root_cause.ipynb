{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, fbeta_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import import_ipynb\n",
    "import data_preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "u4_train_equil_turbine = data_preprocessing.u4_train_equil_turbine\n",
    "u4_train_equil_pump = data_preprocessing.u4_train_equil_pump\n",
    "u5_train_equil_turbine = data_preprocessing.u5_train_equil_turbine\n",
    "u5_train_equil_pump = data_preprocessing.u5_train_equil_pump\n",
    "u6_train_equil_turbine = data_preprocessing.u6_train_equil_turbine\n",
    "u6_train_equil_pump = data_preprocessing.u6_train_equil_pump\n",
    "\n",
    "# synethetic test sets\n",
    "u5_s01_equil_turbine = data_preprocessing.u5_s01_equil_turbine\n",
    "u5_s01_equil_pump = data_preprocessing.u5_s01_equil_pump\n",
    "u5_s02_equil_turbine = data_preprocessing.u5_s02_equil_turbine\n",
    "u5_s02_equil_pump = data_preprocessing.u5_s02_equil_pump\n",
    "u6_s01_equil_turbine = data_preprocessing.u6_s01_equil_turbine\n",
    "u6_s01_equil_pump = data_preprocessing.u6_s01_equil_pump\n",
    "u6_s02_equil_turbine = data_preprocessing.u6_s02_equil_turbine\n",
    "u6_s02_equil_pump = data_preprocessing.u6_s02_equil_pump\n",
    "\n",
    "# real test sets\n",
    "u4_test_equil_turbine = data_preprocessing.u4_test_equil_turbine\n",
    "u4_test_equil_pump = data_preprocessing.u4_test_equil_pump\n",
    "u5_test_equil_turbine = data_preprocessing.u5_test_equil_turbine\n",
    "u5_test_equil_pump = data_preprocessing.u5_test_equil_pump\n",
    "u6_test_equil_turbine = data_preprocessing.u6_test_equil_turbine\n",
    "u6_test_equil_pump = data_preprocessing.u6_test_equil_pump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simplified Autoencoder\n",
    "class FastAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FastAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        reconstruction = self.decoder(feature)\n",
    "        return reconstruction, feature\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data_fast(df):\n",
    "    # Scale data and convert to tensor\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return torch.tensor(scaled_data, dtype=torch.float32), scaler\n",
    "\n",
    "# Optimized training loop\n",
    "def train_autoencoder_fast(model, train_data, val_data, epochs=5, batch_size=32, learning_rate=0.005):\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, _ = model(batch)\n",
    "            loss = criterion(reconstruction, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                reconstruction, _ = model(batch)\n",
    "                loss = criterion(reconstruction, batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.0661, Validation Loss: 0.0460\n",
      "Epoch 2/5, Train Loss: 0.0360, Validation Loss: 0.0339\n",
      "Epoch 3/5, Train Loss: 0.0315, Validation Loss: 0.0298\n",
      "Epoch 4/5, Train Loss: 0.0284, Validation Loss: 0.0259\n",
      "Epoch 5/5, Train Loss: 0.0274, Validation Loss: 0.0261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess u4_train_equil_turbine\n",
    "u4_train_data, scaler = preprocess_data_fast(u4_train_equil_turbine)\n",
    "\n",
    "# Train-validation split\n",
    "train_tensor, val_tensor = train_test_split(u4_train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the autoencoder\n",
    "input_dim = train_tensor.shape[1]\n",
    "model = FastAutoencoder(input_dim)\n",
    "u4_turbine_trained_model = train_autoencoder_fast(model, train_tensor, val_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test data\n",
    "def preprocess_test_data(test_df, train_columns, scaler):\n",
    "    # Reindex test_df to match the columns of the training data\n",
    "    test_df = test_df.reindex(columns=train_columns, fill_value=0)\n",
    "    # Use the same scaler from the training data\n",
    "    scaled_test_data = scaler.transform(test_df)\n",
    "    return torch.tensor(scaled_test_data, dtype=torch.float32)\n",
    "\n",
    "# Evaluate reconstruction error\n",
    "def evaluate_reconstruction(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstruction, _ = model(test_data)\n",
    "        reconstruction_error = torch.mean((reconstruction - test_data) ** 2, dim=1)\n",
    "    return reconstruction_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_anomalies(reconstruction_errors, anomaly_scores, threshold):\n",
    "    \"\"\"\n",
    "    Visualize reconstruction errors and detected anomalies.\n",
    "\n",
    "    Args:\n",
    "        reconstruction_errors (torch.Tensor): Reconstruction errors for the test data.\n",
    "        anomaly_scores (torch.Tensor): Binary tensor indicating anomalies (1 for anomaly, 0 for normal).\n",
    "        threshold (float or torch.Tensor): Threshold value used for anomaly detection.\n",
    "    \"\"\"\n",
    "    # Ensure threshold is a float for consistent handling\n",
    "    if isinstance(threshold, torch.Tensor):\n",
    "        threshold = threshold.item()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(reconstruction_errors.numpy(), label=\"Reconstruction Error\")\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Threshold ({threshold:.4f})\")\n",
    "    plt.scatter(\n",
    "        range(len(anomaly_scores)), \n",
    "        reconstruction_errors.numpy(), \n",
    "        c=anomaly_scores.numpy(), \n",
    "        cmap='coolwarm', \n",
    "        label=\"Anomalies\"\n",
    "    )\n",
    "    plt.title(\"Reconstruction Errors and Detected Anomalies\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Reconstruction Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def define_anomaly_score(reconstruction_errors, threshold):\n",
    "    \"\"\"\n",
    "    Define anomaly scores based on a given threshold.\n",
    "\n",
    "    Args:\n",
    "        reconstruction_errors (torch.Tensor): The reconstruction errors for the test data.\n",
    "        threshold (float): Threshold value for anomaly detection.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Binary tensor indicating anomalies (1 for anomaly, 0 for normal).\n",
    "    \"\"\"\n",
    "    # Generate binary predictions based on the threshold\n",
    "    anomaly_scores = (reconstruction_errors > threshold).int()\n",
    "    return anomaly_scores\n",
    "\n",
    "\n",
    "def custom_score(tp, fp, fn, tn, penalty_fp=2.0):\n",
    "    tp_rate = tp / (tp + fn + 1e-6)\n",
    "    fp_rate = fp / (fp + tn + 1e-6)\n",
    "    return tp_rate - penalty_fp * fp_rate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage for u4_test_equil_turbine\n",
    "# Assume `trained_model` is the autoencoder trained on u4_train_equil_turbine\n",
    "# Assume `scaler` is the StandardScaler fitted to u4_train_equil_turbine\n",
    "\n",
    "# Preprocess test data\n",
    "u4_test_data = preprocess_test_data(u4_test_equil_turbine, u4_train_equil_turbine.columns, scaler)\n",
    "\n",
    "# Evaluate reconstruction errors\n",
    "reconstruction_errors = evaluate_reconstruction(u4_turbine_trained_model, u4_test_data)\n",
    "\n",
    "# Define anomaly scores and threshold\n",
    "# anomaly_scores, threshold = define_anomaly_score(reconstruction_errors)\n",
    "\n",
    "# Visualize anomalies\n",
    "# visualize_anomalies(reconstruction_errors, anomaly_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([18106, 94]), (18106, 94))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u4_test_data.shape, u4_test_equil_turbine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18106])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_errors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u5 s01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.2920, Validation Loss: 0.2250\n",
      "Epoch 2/5, Train Loss: 0.2699, Validation Loss: 0.3959\n",
      "Epoch 3/5, Train Loss: 0.3113, Validation Loss: 0.3211\n",
      "Epoch 4/5, Train Loss: 0.4137, Validation Loss: 0.4034\n",
      "Epoch 5/5, Train Loss: 0.3956, Validation Loss: 0.4035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess u5_train_equil_turbine with scaler validation\n",
    "scaler = StandardScaler()  # Initialize scaler\n",
    "u5_train_turbine_scaled = scaler.fit_transform(u5_train_equil_turbine)  # Fit scaler and transform data\n",
    "u5_train_turbine_data = torch.tensor(u5_train_turbine_scaled, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# Train-validation split\n",
    "train_tensor, val_tensor = train_test_split(u5_train_turbine_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the autoencoder\n",
    "input_dim = train_tensor.shape[1]\n",
    "model = FastAutoencoder(input_dim)\n",
    "u5_turbine_trained_model = train_autoencoder_fast(model, train_tensor, val_tensor)\n",
    "\n",
    "# Ensure that the columns in the test data match the training data\n",
    "u5_s01_turbine_data_preprocessed = preprocess_test_data(u5_s01_equil_turbine, u5_train_equil_turbine.columns, scaler)\n",
    "\n",
    "ground_truth = u5_s01_equil_turbine['anomaly']  # Replace 'anomaly' with the actual column name\n",
    "\n",
    "# Evaluate reconstruction errors\n",
    "reconstruction_errors_u5_turbine = evaluate_reconstruction(u5_turbine_trained_model, u5_s01_turbine_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2726803743827843\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.53      0.53      6092\n",
      "           1       0.46      0.44      0.45      5498\n",
      "           2       0.47      0.43      0.45      5852\n",
      "\n",
      "   micro avg       0.49      0.47      0.48     17442\n",
      "   macro avg       0.49      0.47      0.48     17442\n",
      "weighted avg       0.49      0.47      0.48     17442\n",
      " samples avg       0.32      0.31      0.29     17442\n",
      "\n",
      "[[[4717 2760]\n",
      "  [2861 3231]]\n",
      "\n",
      " [[5302 2769]\n",
      "  [3092 2406]]\n",
      "\n",
      " [[4868 2849]\n",
      "  [3335 2517]]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "X = reconstruction_errors.reshape(-1, 1)\n",
    "y = u5_s01_equil_turbine[['anomaly_01_type_a', 'anomaly_01_type_b', 'anomaly_01_type_c']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "conf_matrices = multilabel_confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u5 s02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Preprocess u5_train_equil_turbine with scaler validation\n",
    "# scaler = StandardScaler()  # Initialize scaler\n",
    "# u5_train_turbine_scaled = scaler.fit_transform(u5_train_equil_turbine)  # Fit scaler and transform data\n",
    "# u5_train_turbine_data = torch.tensor(u5_train_turbine_scaled, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# # Train-validation split\n",
    "# train_tensor, val_tensor = train_test_split(u5_train_turbine_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize and train the autoencoder\n",
    "# input_dim = train_tensor.shape[1]\n",
    "# model = FastAutoencoder(input_dim)\n",
    "# u5_turbine_trained_model = train_autoencoder_fast(model, train_tensor, val_tensor)\n",
    "\n",
    "# # Ensure that the columns in the test data match the training data\n",
    "u5_s02_turbine_data_preprocessed = preprocess_test_data(u5_s02_equil_turbine, u5_train_equil_turbine.columns, scaler)\n",
    "\n",
    "# ground_truth = u5_s01_equil_turbine['anomaly']  # Replace 'anomaly' with the actual column name\n",
    "\n",
    "# Evaluate reconstruction errors\n",
    "reconstruction_errors_u5_s02_turbine = evaluate_reconstruction(u5_turbine_trained_model, u5_s02_turbine_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31229869974949304\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.19      0.20      2882\n",
      "           1       0.64      0.64      0.64      9762\n",
      "           2       0.59      0.57      0.58      9759\n",
      "\n",
      "   micro avg       0.57      0.55      0.56     22403\n",
      "   macro avg       0.48      0.47      0.48     22403\n",
      "weighted avg       0.57      0.55      0.56     22403\n",
      " samples avg       0.48      0.47      0.46     22403\n",
      "\n",
      "[[[11954  1930]\n",
      "  [ 2335   547]]\n",
      "\n",
      " [[ 3521  3483]\n",
      "  [ 3504  6258]]\n",
      "\n",
      " [[ 3123  3884]\n",
      "  [ 4183  5576]]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "X = reconstruction_errors_u5_s02_turbine.reshape(-1, 1)\n",
    "y = u5_s02_equil_turbine[['anomaly_02_type_a', 'anomaly_02_type_b', 'anomaly_02_type_c']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "conf_matrices = multilabel_confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### u6 s01 turbine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.1708, Validation Loss: 0.1186\n",
      "Epoch 2/5, Train Loss: 0.1251, Validation Loss: 0.1194\n",
      "Epoch 3/5, Train Loss: 0.1866, Validation Loss: 0.1560\n",
      "Epoch 4/5, Train Loss: 0.1668, Validation Loss: 0.1743\n",
      "Epoch 5/5, Train Loss: 0.1848, Validation Loss: 0.1877\n"
     ]
    }
   ],
   "source": [
    "# Preprocess u6_train_equil_turbine\n",
    "u6_train_turbine_data, scaler = preprocess_data_fast(u6_train_equil_turbine)\n",
    "\n",
    "# Train-validation split\n",
    "train_tensor, val_tensor = train_test_split(u6_train_turbine_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the autoencoder\n",
    "input_dim = train_tensor.shape[1]\n",
    "model = FastAutoencoder(input_dim)\n",
    "u6_turbine_trained_model = train_autoencoder_fast(model, train_tensor, val_tensor)\n",
    "\n",
    "u6_s01_turbine_data_preprocessed = preprocess_test_data(u6_s01_equil_turbine, u6_train_equil_turbine.columns, scaler)\n",
    "\n",
    "# Evaluate reconstruction errors\n",
    "reconstruction_errors_u6_turbine = evaluate_reconstruction(u6_turbine_trained_model, u6_s01_turbine_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2209089801389058\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.46      0.48      7708\n",
      "           1       0.51      0.49      0.50      8119\n",
      "           2       0.43      0.40      0.41      6054\n",
      "\n",
      "   micro avg       0.48      0.45      0.47     21881\n",
      "   macro avg       0.48      0.45      0.46     21881\n",
      "weighted avg       0.48      0.45      0.47     21881\n",
      " samples avg       0.35      0.34      0.32     21881\n",
      "\n",
      "[[[5052 3654]\n",
      "  [4147 3561]]\n",
      "\n",
      " [[4496 3799]\n",
      "  [4155 3964]]\n",
      "\n",
      " [[7175 3185]\n",
      "  [3650 2404]]]\n"
     ]
    }
   ],
   "source": [
    "X = reconstruction_errors_u6_turbine.reshape(-1,1)\n",
    "y = u6_s01_equil_turbine[['anomaly_01_type_a', 'anomaly_01_type_b', 'anomaly_01_type_c']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "conf_matrices = multilabel_confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "civil-426",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
